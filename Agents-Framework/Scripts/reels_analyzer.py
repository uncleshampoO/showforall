#!/usr/bin/env python3
"""
Reels Analyzer ‚Äî Full pipeline for Instagram Reels analysis
Part of DO Framework: /Scripts/reels_analyzer.py

Pipeline:
1. Parse reels via Apify
2. Download videos & transcribe via Whisper
3. Export to Google Sheets
4. Rank by views
5. Analyze patterns
6. Generate report
"""

import os
import sys
import json
import argparse
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Optional

# Third-party imports
try:
    from apify_client import ApifyClient
    import gspread
    from google.oauth2.service_account import Credentials
except ImportError as e:
    print(f"Missing dependency: {e}")
    print("Run: pip install apify-client gspread google-auth yt-dlp")
    sys.exit(1)

# Load environment variables
from dotenv import load_dotenv

# Constants
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
EXECUTIONS_DIR = PROJECT_ROOT / "Executions"
CREDENTIALS_FILE = PROJECT_ROOT / "Credentials.env"
GOOGLE_CREDS_FILE = PROJECT_ROOT / "google_credentials.json"

# Load credentials
load_dotenv(CREDENTIALS_FILE)

APIFY_TOKEN = os.getenv("APIFY_API_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


class ReelsAnalyzer:
    """Main class for Reels analysis pipeline."""
    
    def __init__(
        self,
        usernames: list[str],
        results_limit: int = 20,
        share_email: str = None,
        whisper_model: str = "base",
        analyze_with_gpt: bool = True
    ):
        self.usernames = usernames
        self.results_limit = results_limit
        self.share_email = share_email
        self.whisper_model = whisper_model
        self.analyze_with_gpt = analyze_with_gpt
        
        self.reels_data = []
        self.spreadsheet_url = None
        self.report_path = None
        
        # Initialize clients
        self.apify_client = ApifyClient(APIFY_TOKEN)
        self.sheets_client = self._init_google_sheets()
    
    def _init_google_sheets(self) -> Optional[gspread.Client]:
        """Initialize Google Sheets client."""
        try:
            scopes = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive"
            ]
            creds = Credentials.from_service_account_file(
                GOOGLE_CREDS_FILE, 
                scopes=scopes
            )
            return gspread.authorize(creds)
        except Exception as e:
            print(f"‚ö†Ô∏è Google Sheets init failed: {e}")
            return None
    
    # =========================================================================
    # STAGE 1: Parse Reels via Apify
    # =========================================================================
    
    def stage1_parse_reels(self) -> list[dict]:
        """Fetch reels data from Instagram via Apify."""
        print("\nüì° STAGE 1: Parsing Instagram Reels via Apify...")
        
        all_reels = []
        
        for username in self.usernames:
            print(f"  ‚Üí Parsing @{username}...")
            
            run_input = {
                "username": [username],
                "resultsLimit": self.results_limit
            }
            
            try:
                run = self.apify_client.actor("apify/instagram-reel-scraper").call(
                    run_input=run_input,
                    timeout_secs=300
                )
                
                # Fetch results
                items = list(self.apify_client.dataset(run["defaultDatasetId"]).iterate_items())
                
                for item in items:
                    reel = {
                        "reel_url": item.get("url", ""),
                        "username": item.get("ownerUsername", username),
                        "caption": item.get("caption", "")[:500],  # Limit length
                        "views": item.get("videoViewCount", 0) or item.get("playCount", 0),
                        "likes": item.get("likesCount", 0),
                        "comments": item.get("commentsCount", 0),
                        "duration": item.get("videoDuration", 0),
                        "hashtags": ", ".join(item.get("hashtags", [])),
                        "timestamp": item.get("timestamp", ""),
                        "video_url": item.get("videoUrl", ""),
                        "transcription": "",  # Will be filled in Stage 2
                        "hook": ""  # First words
                    }
                    all_reels.append(reel)
                
                print(f"    ‚úÖ Found {len(items)} reels")
                
            except Exception as e:
                print(f"    ‚ùå Error parsing @{username}: {e}")
        
        self.reels_data = all_reels
        print(f"\nüìä Total reels collected: {len(all_reels)}")
        return all_reels
    
    # =========================================================================
    # STAGE 2: Transcribe via Whisper
    # =========================================================================
    
    def stage2_transcribe(self) -> None:
        """Download videos and transcribe via local Whisper."""
        print("\nüéôÔ∏è STAGE 2: Transcribing with Whisper...")
        
        # Check Whisper availability
        if not self._check_whisper():
            print("  ‚ùå Whisper not found. Install with: pip install openai-whisper")
            return
        
        total = len(self.reels_data)
        
        for i, reel in enumerate(self.reels_data):
            print(f"  ‚Üí [{i+1}/{total}] Processing {reel['reel_url'][:50]}...")
            
            video_url = reel.get("video_url")
            if not video_url:
                print("    ‚ö†Ô∏è No video URL, skipping")
                continue
            
            try:
                # Download video to temp file
                with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
                    tmp_path = tmp.name
                
                # Use yt-dlp for reliable downloading
                download_cmd = [
                    "yt-dlp",
                    "-f", "worst",  # Smallest file for faster processing
                    "-o", tmp_path,
                    "--quiet",
                    video_url
                ]
                
                subprocess.run(download_cmd, check=True, capture_output=True)
                
                # Transcribe with Whisper
                transcription = self._transcribe_with_whisper(tmp_path)
                reel["transcription"] = transcription
                
                # Extract hook (first ~50 chars)
                if transcription:
                    reel["hook"] = transcription[:100].split(".")[0]
                
                # Cleanup
                os.unlink(tmp_path)
                
                print(f"    ‚úÖ Transcribed: {transcription[:50]}...")
                
            except Exception as e:
                print(f"    ‚ùå Transcription error: {e}")
    
    def _check_whisper(self) -> bool:
        """Check if Whisper is installed."""
        try:
            result = subprocess.run(
                ["whisper", "--help"],
                capture_output=True,
                text=True
            )
            return result.returncode == 0
        except FileNotFoundError:
            return False
    
    def _transcribe_with_whisper(self, audio_path: str) -> str:
        """Transcribe audio file using Whisper CLI."""
        try:
            with tempfile.TemporaryDirectory() as tmp_dir:
                cmd = [
                    "whisper",
                    audio_path,
                    "--model", self.whisper_model,
                    "--output_format", "txt",
                    "--output_dir", tmp_dir,
                    "--language", "ru",  # Can be auto-detected
                    "--task", "transcribe"
                ]
                
                subprocess.run(cmd, check=True, capture_output=True)
                
                # Read output file
                txt_file = Path(tmp_dir) / (Path(audio_path).stem + ".txt")
                if txt_file.exists():
                    return txt_file.read_text().strip()
                
        except Exception as e:
            print(f"      Whisper error: {e}")
        
        return ""
    
    # =========================================================================
    # STAGE 3: Export to Google Sheets
    # =========================================================================
    
    def stage3_export_to_sheets(self) -> str:
        """Export data to Google Sheets."""
        print("\nüìä STAGE 3: Exporting to Google Sheets...")
        
        if not self.sheets_client:
            print("  ‚ùå Google Sheets not initialized")
            return ""
        
        try:
            # Create spreadsheet
            title = f"Reels Analysis {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            spreadsheet = self.sheets_client.create(title)
            
            # Share with user
            if self.share_email:
                spreadsheet.share(self.share_email, perm_type="user", role="writer")
            
            # Prepare Sheet 1: Raw Data
            sheet1 = spreadsheet.sheet1
            sheet1.update_title("Raw Data")
            
            headers = [
                "Reel URL", "Username", "Caption", "Views", "Likes", 
                "Comments", "Duration", "Hashtags", "Timestamp", 
                "Video URL", "Transcription", "Hook"
            ]
            
            rows = [headers]
            for reel in self.reels_data:
                rows.append([
                    reel["reel_url"],
                    reel["username"],
                    reel["caption"],
                    reel["views"],
                    reel["likes"],
                    reel["comments"],
                    reel["duration"],
                    reel["hashtags"],
                    reel["timestamp"],
                    reel["video_url"],
                    reel["transcription"],
                    reel["hook"]
                ])
            
            sheet1.update(range_name="A1", values=rows)
            
            # Create Sheet 2: Ranking
            sheet2 = spreadsheet.add_worksheet(title="Ranking", rows=100, cols=10)
            ranking_data = self._create_ranking_sheet()
            sheet2.update(range_name="A1", values=ranking_data)
            
            self.spreadsheet_url = spreadsheet.url
            print(f"  ‚úÖ Spreadsheet created: {spreadsheet.url}")
            
            return spreadsheet.url
            
        except Exception as e:
            print(f"  ‚ùå Google Sheets error: {e}")
            return ""
    
    def _create_ranking_sheet(self) -> list[list]:
        """Create ranking data sorted by views."""
        headers = ["Rank", "Reel URL", "Views", "Performance", "Hook", "Key Topics"]
        
        # Sort by views
        sorted_reels = sorted(self.reels_data, key=lambda x: x["views"], reverse=True)
        
        total = len(sorted_reels)
        top_threshold = int(total * 0.2)
        bottom_threshold = int(total * 0.8)
        
        rows = [headers]
        for i, reel in enumerate(sorted_reels):
            if i < top_threshold:
                performance = "üèÜ TOP"
            elif i >= bottom_threshold:
                performance = "‚ùå BOTTOM"
            else:
                performance = "‚ûñ MIDDLE"
            
            rows.append([
                i + 1,
                reel["reel_url"],
                reel["views"],
                performance,
                reel["hook"][:50] if reel["hook"] else "",
                self._extract_topics(reel["transcription"])
            ])
        
        return rows
    
    def _extract_topics(self, text: str) -> str:
        """Extract key topics from transcription (simple version)."""
        if not text:
            return ""
        # Simple keyword extraction - can be enhanced with NLP
        words = text.lower().split()
        # Filter common words and return top keywords
        stopwords = {"–∏", "–≤", "–Ω–∞", "—Å", "—á—Ç–æ", "—ç—Ç–æ", "–∫–∞–∫", "–¥–ª—è", "–Ω–µ", "–Ω–æ", "–∞", "—Ç–æ"}
        keywords = [w for w in words if len(w) > 4 and w not in stopwords]
        return ", ".join(set(keywords[:5]))
    
    # =========================================================================
    # STAGE 4: Analyze Patterns
    # =========================================================================
    
    def stage4_analyze_patterns(self) -> dict:
        """Analyze patterns in top and bottom performing reels."""
        print("\nüîç STAGE 4: Analyzing patterns...")
        
        sorted_reels = sorted(self.reels_data, key=lambda x: x["views"], reverse=True)
        
        total = len(sorted_reels)
        top_count = max(5, int(total * 0.2))
        bottom_count = max(5, int(total * 0.2))
        
        top_reels = sorted_reels[:top_count]
        bottom_reels = sorted_reels[-bottom_count:]
        
        analysis = {
            "total_reels": total,
            "total_views": sum(r["views"] for r in self.reels_data),
            "avg_views": sum(r["views"] for r in self.reels_data) // total if total else 0,
            "top_reels": top_reels,
            "bottom_reels": bottom_reels,
            "top_patterns": self._analyze_group(top_reels, "top"),
            "bottom_patterns": self._analyze_group(bottom_reels, "bottom"),
            "recommendations": []
        }
        
        # Generate recommendations based on patterns
        analysis["recommendations"] = self._generate_recommendations(analysis)
        
        print(f"  ‚úÖ Analysis complete")
        return analysis
    
    def _analyze_group(self, reels: list[dict], group_type: str) -> dict:
        """Analyze a group of reels for common patterns."""
        patterns = {
            "avg_duration": 0,
            "common_hooks": [],
            "common_topics": [],
            "avg_engagement_rate": 0,
            "hashtag_patterns": []
        }
        
        if not reels:
            return patterns
        
        # Average duration
        durations = [r["duration"] for r in reels if r["duration"]]
        patterns["avg_duration"] = sum(durations) / len(durations) if durations else 0
        
        # Collect hooks
        patterns["common_hooks"] = [r["hook"] for r in reels if r["hook"]][:5]
        
        # Common hashtags
        all_hashtags = []
        for r in reels:
            if r["hashtags"]:
                all_hashtags.extend(r["hashtags"].split(", "))
        patterns["hashtag_patterns"] = list(set(all_hashtags))[:10]
        
        # Engagement rate (likes/views)
        engagement_rates = []
        for r in reels:
            if r["views"] > 0:
                rate = (r["likes"] / r["views"]) * 100
                engagement_rates.append(rate)
        patterns["avg_engagement_rate"] = (
            sum(engagement_rates) / len(engagement_rates) if engagement_rates else 0
        )
        
        return patterns
    
    def _generate_recommendations(self, analysis: dict) -> list[str]:
        """Generate content recommendations based on analysis."""
        recommendations = []
        
        top_patterns = analysis["top_patterns"]
        bottom_patterns = analysis["bottom_patterns"]
        
        # Duration recommendation
        if top_patterns["avg_duration"]:
            dur = int(top_patterns["avg_duration"])
            recommendations.append(
                f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {dur} —Å–µ–∫—É–Ω–¥ (—Å—Ä–µ–¥–Ω–µ–µ —É –¢–û–ü reels)"
            )
        
        # Hooks recommendation
        if top_patterns["common_hooks"]:
            recommendations.append(
                f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ hooks: –Ω–∞—á–∏–Ω–∞—Ç—å —Å –≤–æ–ø—Ä–æ—Å–∞ –∏–ª–∏ –∏–Ω—Ç—Ä–∏–≥–∏"
            )
        
        # Engagement recommendation
        if top_patterns["avg_engagement_rate"] > bottom_patterns["avg_engagement_rate"]:
            diff = top_patterns["avg_engagement_rate"] - bottom_patterns["avg_engagement_rate"]
            recommendations.append(
                f"–¢–û–ü reels –∏–º–µ—é—Ç engagement rate –≤—ã—à–µ –Ω–∞ {diff:.1f}%"
            )
        
        return recommendations
    
    # =========================================================================
    # STAGE 5: Generate Report
    # =========================================================================
    
    def stage5_generate_report(self, analysis: dict) -> str:
        """Generate markdown report."""
        print("\nüìù STAGE 5: Generating report...")
        
        date_str = datetime.now().strftime("%Y-%m-%d")
        time_str = datetime.now().strftime("%H:%M")
        username_str = "_".join(self.usernames)
        
        report_filename = f"{date_str}_reels_analysis_{username_str}.md"
        report_path = EXECUTIONS_DIR / report_filename
        
        # Ensure Executions directory exists
        EXECUTIONS_DIR.mkdir(exist_ok=True)
        
        report = f"""# üìä –ê–Ω–∞–ª–∏–∑ Instagram Reels: @{', @'.join(self.usernames)}

**–î–∞—Ç–∞:** {date_str} {time_str}  
**–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** {analysis['total_reels']} reels  
**Google Sheets:** {self.spreadsheet_url or '–ù–µ —Å–æ–∑–¥–∞–Ω'}

---

## üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |
|---------|----------|
| –í—Å–µ–≥–æ reels | {analysis['total_reels']} |
| –°—É–º–º–∞—Ä–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã | {analysis['total_views']:,} |
| –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã | {analysis['avg_views']:,} |

---

## üèÜ –¢–û–ü-5 Reels (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)

"""
        
        for i, reel in enumerate(analysis["top_reels"][:5], 1):
            report += f"""### {i}. {reel['views']:,} views
- **Hook:** "{reel['hook'][:100]}"
- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {reel['duration']} —Å–µ–∫
- **–°—Å—ã–ª–∫–∞:** {reel['reel_url']}

"""
        
        report += """---

## ‚ùå BOTTOM-5 Reels (—á—Ç–æ –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç)

"""
        
        for i, reel in enumerate(analysis["bottom_reels"][:5], 1):
            report += f"""### {i}. {reel['views']:,} views
- **Hook:** "{reel['hook'][:100]}"
- **–ü—Ä–æ–±–ª–µ–º—ã:** –ù–∏–∑–∫–∞—è –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç—å
- **–°—Å—ã–ª–∫–∞:** {reel['reel_url']}

"""
        
        report += f"""---

## üéØ –ü–∞—Ç—Ç–µ—Ä–Ω—ã —É—Å–ø–µ—Ö–∞

### Hooks, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç:
"""
        for hook in analysis["top_patterns"]["common_hooks"][:3]:
            report += f'- "{hook}"\n'
        
        report += f"""
### –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: ~{int(analysis['top_patterns']['avg_duration'])} —Å–µ–∫—É–Ω–¥
- Engagement rate: {analysis['top_patterns']['avg_engagement_rate']:.1f}%

---

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É

"""
        for rec in analysis["recommendations"]:
            report += f"- {rec}\n"
        
        report += """
---

## üó≥Ô∏è –í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ, –ø—Ä–µ–¥–ª–∞–≥–∞—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

1. **–ü–æ–≤—Ç–æ—Ä–∏—Ç—å —É—Å–ø–µ—à–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç** ‚Äî —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –¢–û–ü reels
2. **–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é** ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –¥–ª–∏–Ω—É
3. **–ù–æ–≤–∞—è —Ç–µ–º–∞** ‚Äî –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

**–ö–∞–∫–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏—è? (1/2/3/—Å–≤–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)**
"""
        
        # Write report
        report_path.write_text(report, encoding="utf-8")
        self.report_path = report_path
        
        print(f"  ‚úÖ Report saved: {report_path}")
        return str(report_path)
    
    # =========================================================================
    # STAGE 6 & 7: Interactive (handled by agent externally)
    # =========================================================================
    
    def run_full_pipeline(self) -> dict:
        """Run complete analysis pipeline."""
        print("=" * 60)
        print("üé¨ REELS ANALYZER ‚Äî Starting Full Pipeline")
        print("=" * 60)
        
        # Stage 1: Parse
        self.stage1_parse_reels()
        
        if not self.reels_data:
            print("\n‚ùå No reels found. Aborting.")
            return {"success": False, "error": "No reels found"}
        
        # Stage 2: Transcribe
        self.stage2_transcribe()
        
        # Stage 3: Export to Sheets
        self.stage3_export_to_sheets()
        
        # Stage 4: Analyze
        analysis = self.stage4_analyze_patterns()
        
        # Stage 5: Generate Report
        report_path = self.stage5_generate_report(analysis)
        
        print("\n" + "=" * 60)
        print("‚úÖ PIPELINE COMPLETE")
        print("=" * 60)
        print(f"\nüìä Google Sheets: {self.spreadsheet_url}")
        print(f"üìù Report: {report_path}")
        print("\n‚è≥ Stage 6-7 (feedback & script) handled by agent interactively")
        
        return {
            "success": True,
            "reels_count": len(self.reels_data),
            "spreadsheet_url": self.spreadsheet_url,
            "report_path": str(report_path),
            "analysis": analysis
        }


def main():
    parser = argparse.ArgumentParser(description="Instagram Reels Analyzer")
    parser.add_argument(
        "--usernames", "-u",
        required=True,
        help="Comma-separated Instagram usernames"
    )
    parser.add_argument(
        "--limit", "-l",
        type=int,
        default=20,
        help="Max reels per profile (default: 20)"
    )
    parser.add_argument(
        "--email", "-e",
        required=True,
        help="Email for Google Sheets access"
    )
    parser.add_argument(
        "--whisper-model", "-w",
        default="base",
        choices=["tiny", "base", "small", "medium", "large"],
        help="Whisper model size (default: base)"
    )
    parser.add_argument(
        "--no-gpt",
        action="store_true",
        help="Disable GPT-based analysis"
    )
    
    args = parser.parse_args()
    
    usernames = [u.strip() for u in args.usernames.split(",")]
    
    analyzer = ReelsAnalyzer(
        usernames=usernames,
        results_limit=args.limit,
        share_email=args.email,
        whisper_model=args.whisper_model,
        analyze_with_gpt=not args.no_gpt
    )
    
    result = analyzer.run_full_pipeline()
    
    if result["success"]:
        print("\n‚úÖ Done! Check your Google Sheets and report file.")
    else:
        print(f"\n‚ùå Failed: {result.get('error', 'Unknown error')}")
        sys.exit(1)


if __name__ == "__main__":
    main()
